{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7c5fde",
   "metadata": {},
   "source": [
    "# Mapping Values with Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c488b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/rl3725/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3faa2002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-17 13:09:56,019] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24fedef365f48da82f57520ad020aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/vast/work/public/ml-datasets/llama-2/Llama-2-13b-chat-hf/\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/vast/work/public/ml-datasets/llama-2/Llama-2-13b-chat-hf/\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea369e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "You are an intelligent system that given a term, you have to choose a value from a list that best matches the term. \n",
      "These terms belong to the medical domain, and the list contains terms in the Genomics Data Commons (GDC) format.<</SYS>>\n",
      "\n",
      "Can you help me? [/INST]\n",
      "  Yes, I can certainly help! I have been trained on a wide range of medical terminology, including genomics data. Please provide the term you would like me to match, and I will do my best to choose the most appropriate value from the GDC format list.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are an intelligent system that given a term, you have to choose a value from a list that best matches the term. \n",
    "These terms belong to the medical domain, and the list contains terms in the Genomics Data Commons (GDC) format.<</SYS>>\n",
    "\n",
    "Can you help me? [/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36415d74-79b0-4697-9d14-35ddb55b8a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "For the term: \"pt1a (figo ia)\", choose a value from this list ['t0', 't1', 't1a', 't1a1', 't1a2', 't1b', 't1b1', 't1b2', 't1c', 't1c2', 't1mi', 't2', 't2a', 't2a1', 't2a2', 't2b', 't2c', 't2d', 't3', 't3a', 't3b', 't3c', 't3d', 't4', 't4a', 't4b', 't4c', 't4d', 't4e', 'ta', 'tis', 'tis (dcis)', 'tis (lcis)', \"tis (paget's)\", 'tx', 'unknown', 'not reported', 'not allowed to collect'].  <</SYS>>\n",
      "[/INST]\n",
      " Sure! Based on the term \"pt1a (figo ia)\", I would choose the value \"t1a\".</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "For the term: \"pt1a (figo ia)\", choose a value from this list ['t0', 't1', 't1a', 't1a1', 't1a2', 't1b', 't1b1', 't1b2', 't1c', 't1c2', 't1mi', 't2', 't2a', 't2a1', 't2a2', 't2b', 't2c', 't2d', 't3', 't3a', 't3b', 't3c', 't3d', 't4', 't4a', 't4b', 't4c', 't4d', 't4e', 'ta', 'tis', 'tis (dcis)', 'tis (lcis)', \"tis (paget's)\", 'tx', 'unknown', 'not reported', 'not allowed to collect'].  <</SYS>>\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c5e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "For the term: \"figo grade 1\", choose a value from this list ['g1', 'g2', 'g3', 'g4', 'gb', 'gx', 'high grade', 'intermediate grade', 'low grade', 'unknown', 'not reported'].  <</SYS>>\n",
      "[/INST]\n",
      " Sure! Based on the term \"figo grade 1\", I would choose the value \"g1\".</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "For the term: \"figo grade 1\", choose a value from this list ['g1', 'g2', 'g3', 'g4', 'gb', 'gx', 'high grade', 'intermediate grade', 'low grade', 'unknown', 'not reported'].  <</SYS>>\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10095cc9-b00d-4641-bbc7-34fc89ae8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "For the term: \"pn0\", choose a value from this list ['n0', 'n0 (i+)', 'n0 (i-)', 'n0 (mol+)', 'n0 (mol-)', 'n1', 'n1a', 'n1b', 'n1bi', 'n1bii', 'n1biii', 'n1biv', 'n1c', 'n1mi', 'n2', 'n2a', 'n2b', 'n2c', 'n2mi', 'n3', 'n3a', 'n3b', 'n3c', 'n4', 'nx', 'unknown', 'not reported', 'not allowed to collect'].   <</SYS>>\n",
      "[/INST]\n",
      " Sure! I choose 'n0'.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "For the term: \"pn0\", choose a value from this list ['n0', 'n0 (i+)', 'n0 (i-)', 'n0 (mol+)', 'n0 (mol-)', 'n1', 'n1a', 'n1b', 'n1bi', 'n1bii', 'n1biii', 'n1biv', 'n1c', 'n1mi', 'n2', 'n2a', 'n2b', 'n2c', 'n2mi', 'n3', 'n3a', 'n3b', 'n3c', 'n4', 'nx', 'unknown', 'not reported', 'not allowed to collect'].   <</SYS>>\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae9fa36-5be8-4d17-8328-0c5355ab6e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "For the term: \"endometrioid\", choose a value from this list ['8000/0', '8010/0', '8010/2', '8010/3', '8010/6', '8010/9', '8011/0', '8011/3', '8012/3', '8013/3', '8014/3', '8015/3', '8020/3', '8020/6', '8021/3', '8022/3', '8023/3', '8030/3', '8031/3', '8032/3', '8033/3', '8034/3', '8035/3', '8040/0', '8040/1', '8040/3', '8041/3', '8041/6', '8041/34', '8042/3', '8043/3', '8044/3'].  <</SYS>>\n",
      "[/INST]\n",
      " Sure! I choose the value '8010/2'.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "For the term: \"endometrioid\", choose a value from this list ['8000/0', '8010/0', '8010/2', '8010/3', '8010/6', '8010/9', '8011/0', '8011/3', '8012/3', '8013/3', '8014/3', '8015/3', '8020/3', '8020/6', '8021/3', '8022/3', '8023/3', '8030/3', '8031/3', '8032/3', '8033/3', '8034/3', '8035/3', '8040/0', '8040/1', '8040/3', '8041/3', '8041/6', '8041/34', '8042/3', '8043/3', '8044/3'].  <</SYS>>\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6b6ecb-5aeb-45fd-b8c5-d169dbafa488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "Why did you choose the value '8010/2'? <</SYS>>\n",
      "[/INST]\n",
      "I apologize for any confusion. I did not choose the value '8010/2'. This is a random value that was generated by the system for the purpose of this example.\n",
      "\n",
      "In general, the value of the '8010/2' is not a meaningful or important value in the context of the example. It is simply a random value that was used to illustrate the concept of a system of linear equations.\n",
      "\n",
      "If you have any specific questions or concerns about the example, I would be happy to help answer them to the best of my ability. Please let me know if there is anything else you would like to know.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Why did you choose the value '8010/2'? <</SYS>>\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ceb30e-62e8-4bc2-a7b3-6a6d4ece69be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
